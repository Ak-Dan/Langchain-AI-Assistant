
# ğŸ“š LangChain RAG Assistant

A **Retrieval-Augmented Generation (RAG)** assistant built with **LangChain**, **FAISS**, and **LLMs** (Groq, OpenAI, or Google Gemini).
It uses downloaded portions of the LangChain documentation as its knowledge base and answers technical questions with context-aware responses.

---

## âœ¨ Features

* ğŸ” **Local Knowledge Base** â€” uses a portion of LangChain documentation stored in a local `data/` folder.
* ğŸ§© **Document Chunking** â€” splits large files into manageable text chunks for better retrieval.
* âš¡ **FAISS Vector Database** â€” fast, efficient similarity search on embeddings.
* ğŸ§  **Multiple LLM Backends** â€” works with Groq, OpenAI, or Google Gemini depending on available API keys.
* ğŸ¤– **Interactive Q&A Loop** â€” ask questions and get answers grounded in documentation context.
* ğŸ›¡ **Safe & Relevant Answers** â€” refuses unsafe or unrelated queries and avoids hallucinations.

---

## ğŸ—ï¸ Project Architecture

The project is organized into three main components:

### 1. `download_docs.py` (Data Preparation)

* Downloads a portion of the **LangChain documentation**.
* Saves `.txt` files locally in the `data/` folder.
* Only needs to be run if you want to refresh or update the docs.

### 2. `vectordb.py` (Vector Database)

* Splits documents into smaller **chunks** using a text splitter.
* Generates embeddings using **SentenceTransformers** (`all-MiniLM-L6-v2`).
* Stores the embeddings in a **FAISS** database for fast similarity search.
* Provides methods to:

  * Add new documents into the DB.
  * Query for the most relevant chunks given a question.

### 3. `app.py` (RAG Assistant)

* Initializes an LLM (Groq, OpenAI, or Google Gemini).
* Retrieves relevant context from FAISS.
* Injects retrieved context into a **prompt template**.
* Passes the prompt to the LLM to generate a context-aware answer.
* Runs an **interactive Q&A loop** where users can type questions.

ğŸ“Œ **Data Flow:**

```mermaid
flowchart TD
    A[download_docs.py] --> B[data/ folder]
    B --> C[vectordb.py: chunk & embed]
    C --> D[FAISS Vector Store]
    D --> E[app.py: RAG Assistant]
    E --> F[LLM Response]
```

---

## âš™ï¸ Installation

Clone the repository and install dependencies:

```bash
git clone https://github.com/Ak-Dan/langchain-rag-assistant.git
cd langchain-rag-assistant
pip install -r requirements.txt
```

---

## ğŸ”‘ Setup

1. Create a `.env` file in the project root with at least one API key:

```env
# Choose one or more
OPENAI_API_KEY=your_openai_api_key_here
GROQ_API_KEY=your_groq_api_key_here
GOOGLE_API_KEY=your_google_api_key_here

# Optional: model names
OPENAI_MODEL=gpt-4o-mini
GROQ_MODEL=llama-3.1-8b-instant
GOOGLE_MODEL=gemini-2.0-flash
```

2. Make sure the **LangChain docs** are available:

   * Either run `download_docs.py` to fetch them.
   * Or manually place `.txt` files into the `data/` folder.

---

## ğŸš€ Usage

Run the assistant:

```bash
python app.py
```

Youâ€™ll see output like this:

```
Initializing RAG Assistant...
Using Groq model: llama-3.1-8b-instant
Loading embedding model: sentence-transformers/all-MiniLM-L6-v2
Vector database initialized with FAISS
RAG Assistant initialized successfully

Loading documents...
Loaded 13 sample documents
Prepared 486 chunks. Encoding embeddings...
All documents added to vector database
```

Then, interact with the assistant:

```
Enter a question or 'quit' to exit: What is LangChain?
```

---

## ğŸ“‚ Project Structure

```
â”œâ”€â”€ app.py              # Main RAG Assistant
â”œâ”€â”€ vectordb.py         # Vector DB (chunking + embeddings + FAISS)
â”œâ”€â”€ download_docs.py    # Script to download LangChain docs
â”œâ”€â”€ data/               # Folder containing downloaded docs (.txt)
â”œâ”€â”€ requirements.txt    # Dependencies
â”œâ”€â”€ .env                # API keys
â””â”€â”€ README.md           # Documentation
```

---

## ğŸ“Œ Notes

* If you already have `.txt` files in the `data/` folder, you **donâ€™t need** to run `download_docs.py`.
* FAISS is **cross-platform** and avoids the metadata mismatch issues seen with ChromaDB on Windows.
* You can easily swap FAISS for another vector store if needed (e.g., Pinecone, Weaviate).

---

## ğŸ› ï¸ Tech Stack

* **LangChain** â€“ Orchestration framework
* **FAISS** â€“ Vector database for embeddings
* **SentenceTransformers** â€“ Embedding model (`all-MiniLM-L6-v2`)
* **Groq / OpenAI / Google Gemini** â€“ LLM backends
* **Python 3.9+**

---

## ğŸ“– Example Interaction

```
Enter a question or 'quit' to exit: What is a LangChain Chain?

Answer:
A LangChain Chain is a sequence of calls where the output of one step is the input to the next.
Chains allow developers to build more complex workflows by linking multiple components together.
```

---

## ğŸ“œ License

This project is licensed under the **MIT License**.

---

